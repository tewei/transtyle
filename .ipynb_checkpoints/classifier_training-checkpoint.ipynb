{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Documents\n",
    "import csv\n",
    "import string\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "train_raw_neg = []\n",
    "train_raw_pos = []\n",
    "\n",
    "with open('data/yelp/sentiment.train.0', 'rb') as fp:\n",
    "    train_raw_neg = fp.readlines()\n",
    "    \n",
    "with open('data/yelp/sentiment.train.1', 'rb') as fp:\n",
    "    train_raw_pos = fp.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177218\n",
      "266041\n"
     ]
    }
   ],
   "source": [
    "neg_sent_cnt = len(train_raw_neg)\n",
    "pos_sent_cnt = len(train_raw_pos)\n",
    "print neg_sent_cnt\n",
    "print pos_sent_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "\n",
    "stop_word_list = list(stopwords.words('english'))\n",
    "\n",
    "def tokenize_std(content, n_gram):\n",
    "    content = content.lower()\n",
    "    content = content.replace(\"\\\\n\", \" \")\n",
    "    words = content.split()\n",
    "    tokens = []\n",
    "    for i in range(n_gram):\n",
    "        for j in range(len(words)-i):\n",
    "            tokens.append(\"_\".join(words[j:j+i+1]))\n",
    "    \n",
    "    tokens = [w for w in tokens if not w in stop_word_list]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_token_neg = []\n",
    "train_token_pos = []\n",
    "\n",
    "for s in train_raw_neg:\n",
    "    train_token_neg.append(tokenize_std(s, 2))\n",
    "\n",
    "for s in train_raw_pos:\n",
    "    train_token_pos.append(tokenize_std(s, 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sadly', 'mistaken', '.', 'i_was', 'was_sadly', 'sadly_mistaken', 'mistaken_.'], ['hoagies', ',', 'italian', 'general', 'run', 'mill', '.', 'so_on', 'on_to', 'to_the', 'the_hoagies', 'hoagies_,', ',_the', 'the_italian', 'italian_is', 'is_general', 'general_run', 'run_of', 'of_the', 'the_mill', 'mill_.'], ['minimal', 'meat', 'ton', 'shredded', 'lettuce', '.', 'minimal_meat', 'meat_and', 'and_a', 'a_ton', 'ton_of', 'of_shredded', 'shredded_lettuce', 'lettuce_.'], ['nothing', 'really', 'special', '&', 'worthy', '$', '_num_', 'price', 'tag', '.', 'nothing_really', 'really_special', 'special_&', '&_not', 'not_worthy', 'worthy_of', 'of_the', 'the_$', '$__num_', '_num__price', 'price_tag', 'tag_.'], ['second', ',', 'steak', 'hoagie', ',', 'atrocious', '.', 'second_,', ',_the', 'the_steak', 'steak_hoagie', 'hoagie_,', ',_it', 'it_is', 'is_atrocious', 'atrocious_.'], ['pay', '$', '_num_', 'add', 'cheese', 'hoagie', '.', 'i_had', 'had_to', 'to_pay', 'pay_$', '$__num_', '_num__to', 'to_add', 'add_cheese', 'cheese_to', 'to_the', 'the_hoagie', 'hoagie_.'], ['told', 'charge', 'dressing', 'side', '.', 'she_told', 'told_me', 'me_there', 'there_was', 'was_a', 'a_charge', 'charge_for', 'for_the', 'the_dressing', 'dressing_on', 'on_the', 'the_side', 'side_.'], ['kidding', '?', 'are_you', 'you_kidding', 'kidding_me', 'me_?'], ['going', 'pay', 'dressing', 'side', '.', 'i_was', 'was_not', 'not_going', 'going_to', 'to_pay', 'pay_for', 'for_the', 'the_dressing', 'dressing_on', 'on_the', 'the_side', 'side_.'], ['ordered', 'without', 'lettuce', ',', 'tomato', ',', 'onions', ',', 'dressing', '.', 'i_ordered', 'ordered_it', 'it_without', 'without_lettuce', 'lettuce_,', ',_tomato', 'tomato_,', ',_onions', 'onions_,', ',_or', 'or_dressing', 'dressing_.']]\n",
      "[['excellent', 'food', '.', 'excellent_food', 'food_.'], ['superb', 'customer', 'service', '.', 'superb_customer', 'customer_service', 'service_.'], ['also', 'daily', 'specials', 'ice', 'cream', 'really', 'good', '.', 'they_also', 'also_have', 'have_daily', 'daily_specials', 'specials_and', 'and_ice', 'ice_cream', 'cream_which', 'which_is', 'is_really', 'really_good', 'good_.'], [\"'s\", 'good', 'toasted', 'hoagie', '.', \"it_'s\", \"'s_a\", 'a_good', 'good_toasted', 'toasted_hoagie', 'hoagie_.'], ['staff', 'friendly', '.', 'the_staff', 'staff_is', 'is_friendly', 'friendly_.'], ['good', 'bar', 'food', '.', 'good_bar', 'bar_food', 'food_.'], ['good', 'service', '.', 'good_service', 'service_.'], ['soup', 'day', 'homemade', 'lots', 'specials', '.', 'soup_of', 'of_day', 'day_is', 'is_homemade', 'homemade_and', 'and_lots', 'lots_of', 'of_specials', 'specials_.'], ['great', 'place', 'lunch', 'bar', 'snacks', 'beer', '.', 'great_place', 'place_for', 'for_lunch', 'lunch_or', 'or_bar', 'bar_snacks', 'snacks_and', 'and_beer', 'beer_.'], ['new', 'range', 'looks', 'amazing', '.', 'the_new', 'new_range', 'range_looks', 'looks_amazing', 'amazing_.']]\n"
     ]
    }
   ],
   "source": [
    "print train_token_neg[0:10]\n",
    "print train_token_pos[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "unigram_neg = Counter([])\n",
    "unigram_pos = Counter([])\n",
    "unigram_all = Counter([])\n",
    "\n",
    "bigram_neg = Counter([])\n",
    "bigram_pos = Counter([])\n",
    "bigram_all = Counter([])\n",
    "\n",
    "\n",
    "for s in train_token_neg:\n",
    "    for w in s:\n",
    "        bigram_neg[w] += 1\n",
    "        bigram_all[w] += 1\n",
    "    \n",
    "for s in train_token_pos:\n",
    "    for w in s:\n",
    "        bigram_pos[w] += 1\n",
    "        bigram_all[w] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8783\n"
     ]
    }
   ],
   "source": [
    "vocab_list = [w for w in bigram_all if (bigram_all[w] > 72)]\n",
    "vocab_dict = {}\n",
    "idx = 0\n",
    "for w in vocab_list:\n",
    "    vocab_dict[w] = idx\n",
    "    idx += 1\n",
    "\n",
    "print len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "alpha = 1.0\n",
    "d = len(vocab_list)\n",
    "p, q = p, q = np.ones(d) * alpha , np.ones(d) * alpha\n",
    "\n",
    "for w in vocab_list:\n",
    "    p[vocab_dict[w]] += bigram_pos[w]\n",
    "    q[vocab_dict[w]] += bigram_neg[w]\n",
    "\n",
    "p /= abs(p).sum()\n",
    "q /= abs(q).sum()\n",
    "r = np.log(p/q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_all = []\n",
    "Y_all = []\n",
    "for s in train_token_neg:\n",
    "    vec = np.zeros(d)\n",
    "    for w in s:\n",
    "        if w in vocab_dict:\n",
    "            vec[vocab_dict[w]] = r[vocab_dict[w]]\n",
    "\n",
    "    X_all.append(vec)\n",
    "    Y_all.append(-1)\n",
    "\n",
    "for s in train_token_pos:\n",
    "    vec = np.zeros(d)\n",
    "    for w in s:\n",
    "        if w in vocab_dict:\n",
    "            vec[vocab_dict[w]] = r[vocab_dict[w]]\n",
    "\n",
    "    X_all.append(vec)\n",
    "    Y_all.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all[0:30000]+X_all[300000:330000], Y_all[0:30000]+Y_all[300000:330000], test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "#linear NBSVM model\n",
    "clf_svc = LinearSVC(penalty='l2', C=0.1)\n",
    "clf_svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9606666666666667\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('classifier/clf_svc.pickle', 'wb') as f:\n",
    "    pickle.dump(clf_svc, f)\n",
    "    \n",
    "with open('classifier/clf_svc.pickle', 'rb') as f:\n",
    "    clf_svc_o = pickle.load(f)\n",
    "\n",
    "print clf_svc_o.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(C=1.0, penalty='l2', tol=0.001)\n",
    "clf_lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9697\n"
     ]
    }
   ],
   "source": [
    "#with open('classifier/clf_lr.pickle', 'wb') as f:\n",
    "    #pickle.dump(clf_lr, f)\n",
    "    \n",
    "with open('classifier/clf_lr.pickle', 'rb') as f:\n",
    "    clf_lr_o = pickle.load(f)\n",
    "\n",
    "print clf_lr_o.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6766152   1.00147899  0.81081579 ... -0.76472057  0.91358675\n",
      " -2.81290957]\n"
     ]
    }
   ],
   "source": [
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.937251\n"
     ]
    }
   ],
   "source": [
    "#MNB model\n",
    "cnt_true = 0.0\n",
    "cnt_all = len(X_all)\n",
    "for i in range(len(X_all)):\n",
    "    log_prob = np.sum(X_all[i])\n",
    "    if((log_prob > 0 and Y_all[i] == 1) or (log_prob < 0 and Y_all[i] == -1)):\n",
    "        cnt_true += 1\n",
    "        \n",
    "print \"Accuracy: %f\" % (cnt_true/cnt_all)\n",
    "        \n",
    "        \n",
    "def eval_sent_MNB(s):\n",
    "    vec = np.zeros(d)\n",
    "    for w in s:\n",
    "        if w in vocab_dict:\n",
    "            vec[vocab_dict[w]] = r[vocab_dict[w]]\n",
    "            \n",
    "    return [np.sum(vec)]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_sent_svm(clf, s):\n",
    "    \n",
    "    vec = np.zeros(d)\n",
    "    for w in s:\n",
    "        if w in vocab_dict:\n",
    "            vec[vocab_dict[w]] = r[vocab_dict[w]]\n",
    "        \n",
    "    #print clf.predict([vec])\n",
    "    return clf.decision_function([vec])\n",
    "\n",
    "def eval_sent_lr(clf, s):\n",
    "    \n",
    "    vec = np.zeros(d)\n",
    "    for w in s:\n",
    "        if w in vocab_dict:\n",
    "            vec[vocab_dict[w]] = r[vocab_dict[w]]\n",
    "            \n",
    "    return clf.predict_proba([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models\n"
     ]
    }
   ],
   "source": [
    "#load word2vec\n",
    "from gensim import models\n",
    "print('Loading models')\n",
    "model = models.Word2Vec.load('sentiment_train_0.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantastic,0.84588009119\n",
      "wonderful,0.829598665237\n",
      "terrific,0.828809738159\n",
      "good,0.795733213425\n",
      "fabulous,0.763466656208\n",
      "excellent,0.747741580009\n",
      "decent,0.713745057583\n",
      "awesome,0.712673127651\n",
      "superb,0.711408376694\n",
      "nice,0.701542019844\n",
      "perfect,0.697403073311\n",
      "lovely,0.665784835815\n",
      "amazing,0.662964224815\n",
      "phenomenal,0.659969866276\n",
      "terrible,0.650196731091\n",
      "outstanding,0.6210744977\n",
      "alright,0.611912190914\n",
      "horrible,0.601214408875\n",
      "incredible,0.596231222153\n",
      "fine,0.592000186443\n"
     ]
    }
   ],
   "source": [
    "res = model.most_similar('great',topn = 20)\n",
    "for item in res:\n",
    "    print(item[0]+\",\"+str(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"n't\", 'RB') 1.698232503772196\n",
      "even though the groupon included menu items , they do n't have a kitchen .\n",
      "even though the groupon included menu items , they do  have a kitchen .\n",
      "('never', 'RB') 1.6161718212705147\n",
      "('going', 'VBG') 1.7318410286539985\n",
      "('back', 'RB') 1.7693337474057196\n",
      "i am never going back to this place .\n",
      "i am  planning  to this place .\n",
      "('disappointed', 'VBN') 1.786088623323813\n",
      "very disappointed that this is the only photo store in the city .\n",
      "very  that this is the only photo store in the city .\n",
      "('unfortunately', 'RB') 2.332721425952831\n",
      "unfortunately , this is the only game in town .\n",
      "apparently , this is the only game in town .\n",
      "('gave', 'VBD') 1.5746631267088567\n",
      "they gave me a nice receipt .\n",
      "they saw me a nice receipt .\n",
      "('lackluster', 'JJ') 1.2760087742457873\n",
      "service and products are lackluster .\n",
      "service and products are pitiful .\n",
      "overall , not a great choice .\n",
      "overall , not a great choice .\n",
      "('hung', 'VBP') 1.127615946454887\n",
      "in addition to being condescending , they hung up on me .\n",
      "in addition to being condescending , they  up on me .\n",
      "(\"n't\", 'RB') 1.63786056904903\n",
      "('quite', 'RB') 1.5896556237399608\n",
      "('accurate', 'JJ') 1.5797766082605675\n",
      "actually that is n't quite accurate .\n",
      "actually that is  incredibly attractive .\n",
      "neither one greeted me or asked me what i would have like to order .\n",
      "neither one greeted me or asked me what i would have like to order .\n",
      "(\"n't\", 'RB') 1.63786056904903\n",
      "the food is n't fresh .\n",
      "the food is  fresh .\n",
      "(\"n't\", 'RB') 1.4600562201108396\n",
      "('pan', 'JJ') 1.4859109204545704\n",
      "('fried', 'VBN') 1.6080533825243801\n",
      "it was n't pan fried either .\n",
      "it was  dish  either .\n",
      "('awful', 'JJ') 2.5958200002639966\n",
      "my sides , german potato salad and braised red cabbage , were equally awful .\n",
      "my sides , german potato salad and braised red cabbage , were equally terrific .\n",
      "(\"n't\", 'RB') 2.1680827088231567\n",
      "('bite', 'JJ') 3.11832928157625\n",
      "i did n't have more than a bite or two .\n",
      "i did  have more than a  or two .\n",
      "('mediocre', 'JJ') 1.592912199199714\n",
      "('best', 'JJS') 2.0077315390253863\n",
      "food was mediocre at best .\n",
      "food was over-priced at finest .\n",
      "('used', 'VBN') 1.8048235954413752\n",
      "what was set in front me looked nothing like what i was used to .\n",
      "what was set in front me looked nothing like what i was owned to .\n",
      "('sadly', 'RB') 2.5739381764797473\n",
      "(\"n't\", 'RB') 4.2511164514845605\n",
      "('like', 'VB') 3.1631943226412784\n",
      "sadly , i just did n't like it .\n",
      "thankfully , i just did not  it .\n",
      "i thought it was actually not very appetizing at all .\n",
      "i thought it was actually not very appetizing at all .\n",
      "('disappointed', 'VBN') 1.031962270715488\n",
      "i was highly disappointed .\n",
      "i was highly  .\n",
      "('bad', 'JJ') 1.2830985395507462\n",
      "(\"n't\", 'RB') 3.1068525824226256\n",
      "('drink', 'VB') 3.114336062315221\n",
      "('kind', 'VB') 2.971976420358483\n",
      "('bad', 'JJ') 4.497761619370533\n",
      "it was bad ... the you ca n't drink it kind of bad .\n",
      "it was good ... the you ca not  it  of harsh .\n",
      "('disappointed', 'VBN') 1.4381444593783068\n",
      "my husband has eaten there before and was sorely disappointed as well .\n",
      "my husband has eaten there before and was sorely  as well .\n",
      "('instead', 'RB') 1.5356277791117796\n",
      "('frozen', 'VBN') 1.9359336996472605\n",
      "it was made from pork instead of beef with frozen peas and carrots .\n",
      "it was made from pork  of beef with  peas and carrots .\n",
      "('tired', 'JJ') 1.3646640270089518\n",
      "the restaurant is located in a tired part of town .\n",
      "the restaurant is located in a  part of town .\n",
      "('cold', 'JJ') 1.5005212082884807\n",
      "for starters , the food is always served cold .\n",
      "for starters , the food is always served hot .\n",
      "(\"n't\", 'RB') 1.908685731548072\n",
      "('believe', 'VB') 1.8587448046673278\n",
      "do n't believe me ?\n",
      "do   me ?\n",
      "('undercooked', 'VBN') 1.0062656775944243\n",
      "yesterday they were undercooked , gritty , and gross .\n",
      "yesterday they were  , gritty , and gross .\n",
      "('bland', 'VBP') 2.2813472260807686\n",
      "('stop', 'VB') 2.2817728803930093\n",
      "if you want bland tasteless food , then stop here .\n",
      "if you want  tasteless food , then  here .\n",
      "('need', 'VBP') 1.1687045297022516\n",
      "('get', 'VB') 1.3164374548956033\n",
      "they need to get out of the _num_ 's with the decor .\n",
      "they  to take out of the _num_ 's with the decor .\n",
      "(\"n't\", 'RB') 2.32944679496026\n",
      "('even', 'RB') 2.191375766542456\n",
      "('make', 'VB') 2.2591536279981463\n",
      "then , they did n't even do anything to make it up to me .\n",
      "then , they did   do anything to provide it up to me .\n",
      "('even', 'RB') 1.2350958639890284\n",
      "not even the potato pancakes tasted good .\n",
      "not  the potato pancakes tasted good .\n",
      "who knows how long that sour cream had been sitting out !\n",
      "who knows how long that sour cream had been sitting out !\n",
      "not a fan of this `` german '' place .\n",
      "not a fan of this `` german '' place .\n",
      "('awful', 'JJ') 2.5958200002639966\n",
      "simply awful .\n",
      "simply terrific .\n",
      "('low', 'JJ') 2.140918066777966\n",
      "very low quality food for the price .\n",
      "very reasonable quality food for the price .\n",
      "the only good part was the bacon , which was ok at best .\n",
      "the only good part was the bacon , which was ok at best .\n",
      "i have not a single good thing to say about this restaurant .\n",
      "i have not a single good thing to say about this restaurant .\n",
      "('terrible', 'JJ') 2.5146076946271556\n",
      "('took', 'VBD') 3.728685570525318\n",
      "('get', 'VB') 3.9769072819768287\n",
      "the service was terrible , took _num_ min to get a water .\n",
      "the service was impeccable , lasted _num_ min to take a water .\n",
      "('avoid', 'VBD') 1.4780991547485383\n",
      "avoid at all cost .\n",
      " at all cost .\n",
      "('disappointed', 'VBD') 1.031962270715488\n",
      "disappointed .\n",
      "surprised .\n",
      "(\"n't\", 'RB') 1.463037686424083\n",
      "it should n't be this way .\n",
      "it should  be this way .\n",
      "('special', 'JJ') 1.5591952238484796\n",
      "even without grumpy old man , there 's nothing special about this place .\n",
      "even without grumpy old man , there 's nothing spectacular about this place .\n",
      "('sub-par', 'JJ') 1.147216693292159\n",
      "everything else was sub-par .\n",
      "everything else was impeccable .\n",
      "('bad', 'JJ') 1.5586165893221116\n",
      "the hotel is in a bad neighborhood .\n",
      "the hotel is in a good neighborhood .\n",
      "('bad', 'JJ') 1.7049445695086187\n",
      "('overall', 'JJ') 2.7607296021026473\n",
      "just bad service overall .\n",
      "just good service  .\n",
      "('rude', 'VBP') 2.523882771428377\n",
      "even the day front desk person was short with us and rude .\n",
      "even the day front desk person was short with us and  .\n",
      "(\"n't\", 'RB') 1.0126356178610498\n",
      "('wait', 'VB') 1.1869603337995924\n",
      "('get', 'VB') 1.3346932589929439\n",
      "i could n't wait to get out of there .\n",
      "i could   to take out of there .\n",
      "(\"n't\", 'RB') 1.0448644292394502\n",
      "('stay', 'VB') 1.0004307479307788\n",
      "i would n't stay here again .\n",
      "i would  keep here again .\n",
      "('never', 'RB') 1.2018870081987596\n",
      "('annoying', 'VBG') 2.87033454403953\n",
      "we never got a call about the reservation time which was annoying .\n",
      "we  got a call about the reservation time which was disturbing .\n",
      "('refused', 'VBD') 1.0977471519043909\n",
      "('call', 'VB') 1.3860170742126143\n",
      "('talk', 'VB') 1.3766343390717528\n",
      "then refused to call m & p to talk to the manager .\n",
      "then managed to  m & p to  to the manager .\n",
      "('terrible', 'JJ') 2.1470766997751127\n",
      "('barely', 'RB') 4.020698767862239\n",
      "('walk', 'VB') 2.9216722722145607\n",
      "i have terrible foot pain and can barely walk .\n",
      "i have impeccable foot pain and can hardly jump .\n",
      "('disappointed', 'VBN') 1.7860886233238125\n",
      "('seen', 'VBN') 1.9580317057809786\n",
      "very disappointed that i can not be seen sooner .\n",
      "very  that i can not be observed sooner .\n",
      "do not believe he cares about his patients .\n",
      "do not believe he cares about his patients .\n",
      "(\"n't\", 'RB') 2.0095749270486154\n",
      "('great', 'JJ') 1.7234078799572743\n",
      "the italian and mushroom steak hoagies were n't that great .\n",
      "the italian and mushroom steak hoagies were  that beautiful .\n",
      "('falling', 'VBG') 1.0494890613708634\n",
      "the steak was so chopped that it kept falling out of the bun .\n",
      "the steak was so chopped that it kept  out of the bun .\n",
      "(\"n't\", 'RB') 2.347890901797406\n",
      "('good', 'JJ') 1.3605952484330222\n",
      "the pizza was n't that good either .\n",
      "the pizza was  that delicious either .\n",
      "i will not get anything from there again .\n",
      "i will not get anything from there again .\n",
      "('bad', 'JJ') 1.5087142222873164\n",
      "the produce in their hoagies is as bad as their salads .\n",
      "the produce in their hoagies is as good as their salads .\n",
      "('better', 'RBR') 1.113530175745248\n",
      "('considering', 'VBG') 1.165821392565511\n",
      "with ron 's , you are better off considering fast food .\n",
      "with ron 's , you are  off  fast food .\n",
      "('mediocre', 'JJ') 1.575211517584643\n",
      "the last _num_ hoagies i 've gotten here is again only mediocre .\n",
      "the last _num_ hoagies i 've gotten here is again only over-priced .\n"
     ]
    }
   ],
   "source": [
    "num_rnds = 1\n",
    "#try_replace = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "\n",
    "try_replace = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "for s in train_raw_neg[2000:2100]:\n",
    "    s_token = nltk.word_tokenize(s)\n",
    "    s_pos = nltk.pos_tag(s_token)\n",
    "    s_len = len(s_token)\n",
    "    \n",
    "    eval_ori = eval_sent_svm(clf_svc, tokenize_std(s, 2))\n",
    "    edit = 0\n",
    "    for i in range(s_len):\n",
    "        if(s_pos[i][1] in try_replace):\n",
    "            s_test = s_token[0:s_len]\n",
    "            s_test[i] = '#'\n",
    "            eval_del = eval_sent_svm(clf_svc, tokenize_std(\" \".join(s_test), 2))\n",
    "            if(eval_del[0] - eval_ori[0] > 1.0):\n",
    "                \n",
    "                edit = 1\n",
    "                if s_token[i] not in vocab_dict:\n",
    "                    continue\n",
    "                print s_pos[i], eval_del[0] - eval_ori[0]\n",
    "                ret = model.most_similar(s_token[i],topn=30)\n",
    "                max_gain_ins = 1.0\n",
    "                to_insert = -1\n",
    "                for k in range(len(ret)):\n",
    "                    pos_test = nltk.pos_tag([ret[k][0]])\n",
    "                    if(pos_test[0][1] == s_pos[i][1] and ret[k][1] > 0.5):\n",
    "                        s_test_ins = s_token[0:s_len]\n",
    "                        s_test_ins[i] = ret[k][0]\n",
    "                        eval_ins = eval_sent_svm(clf_svc, tokenize_std(\" \".join(s_test_ins), 2))\n",
    "                        dif_ins = eval_ins[0] - eval_ori[0]\n",
    "                        if(dif_ins > max_gain_ins):\n",
    "                            max_gain_ins = dif_ins*ret[k][1]\n",
    "                            to_insert = k\n",
    "                \n",
    "                if(to_insert > -1):\n",
    "                    s_token[i] = ret[to_insert][0]\n",
    "                else:\n",
    "                    s_token[i] = \"\"\n",
    "                \n",
    "        \n",
    "    \n",
    "    s_res = \" \".join(s_token)\n",
    "    if(edit == 1):\n",
    "        print s, s_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in train_raw_neg[0:100]:\n",
    "    \n",
    "    \n",
    "    for i in [n_gram-i for i in range(n_gram)]:\n",
    "        for j in range(len(s)-i):\n",
    "            tokens.append(\"_\".join(s[j:j+i+1]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('noticeable', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "test_pos = nltk.pos_tag([\"noticeable\"])\n",
    "print test_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi']\n"
     ]
    }
   ],
   "source": [
    "ex = [\"Hi\", \"There\"]\n",
    "print ex[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
